{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "# to visualise the performance of the model\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# for sequence classification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# for convolution functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WATCH: Six Minutes Of Conservative Media’s Se...</td>\n",
       "      <td>It s no secret that conservatives and Republic...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 2, 2016</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sanders: Firms must take 'haircut' in Puerto R...</td>\n",
       "      <td>WASHINGTON (Reuters) - Wall Street investment ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 1, 2016</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Factbox: Trump fills top jobs for his administ...</td>\n",
       "      <td>(Reuters) - U.S. President-elect Donald Trump ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 29, 2016</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNBC EDITOR: Media Must Remember Readers Are N...</td>\n",
       "      <td>A CNBC editor said members of the press need t...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jun 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYC: Turkish Thugs Beat Up Protesters…Deny Fre...</td>\n",
       "      <td>Remember when these Turkish thugs beat up (see...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Sep 22, 2017</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   WATCH: Six Minutes Of Conservative Media’s Se...   \n",
       "1  Sanders: Firms must take 'haircut' in Puerto R...   \n",
       "2  Factbox: Trump fills top jobs for his administ...   \n",
       "3  CNBC EDITOR: Media Must Remember Readers Are N...   \n",
       "4  NYC: Turkish Thugs Beat Up Protesters…Deny Fre...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  It s no secret that conservatives and Republic...          News   \n",
       "1  WASHINGTON (Reuters) - Wall Street investment ...  politicsNews   \n",
       "2  (Reuters) - U.S. President-elect Donald Trump ...  politicsNews   \n",
       "3  A CNBC editor said members of the press need t...     left-news   \n",
       "4  Remember when these Turkish thugs beat up (see...      politics   \n",
       "\n",
       "                 date label  \n",
       "0      August 2, 2016  fake  \n",
       "1      April 1, 2016   real  \n",
       "2  November 29, 2016   real  \n",
       "3        Jun 29, 2017  fake  \n",
       "4        Sep 22, 2017  fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('data/fake-and-real-news-dataset/combined.csv')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WATCH: Six Minutes Of Conservative Media’s Se...</td>\n",
       "      <td>WATCH: Six Minutes Of Conservative Media’s Se...</td>\n",
       "      <td>News</td>\n",
       "      <td>August 2, 2016</td>\n",
       "      <td>fake</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sanders: Firms must take 'haircut' in Puerto R...</td>\n",
       "      <td>Sanders: Firms must take 'haircut' in Puerto R...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 1, 2016</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Factbox: Trump fills top jobs for his administ...</td>\n",
       "      <td>Factbox: Trump fills top jobs for his administ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 29, 2016</td>\n",
       "      <td>real</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNBC EDITOR: Media Must Remember Readers Are N...</td>\n",
       "      <td>CNBC EDITOR: Media Must Remember Readers Are N...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>Jun 29, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NYC: Turkish Thugs Beat Up Protesters…Deny Fre...</td>\n",
       "      <td>NYC: Turkish Thugs Beat Up Protesters…Deny Fre...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Sep 22, 2017</td>\n",
       "      <td>fake</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   WATCH: Six Minutes Of Conservative Media’s Se...   \n",
       "1  Sanders: Firms must take 'haircut' in Puerto R...   \n",
       "2  Factbox: Trump fills top jobs for his administ...   \n",
       "3  CNBC EDITOR: Media Must Remember Readers Are N...   \n",
       "4  NYC: Turkish Thugs Beat Up Protesters…Deny Fre...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0   WATCH: Six Minutes Of Conservative Media’s Se...          News   \n",
       "1  Sanders: Firms must take 'haircut' in Puerto R...  politicsNews   \n",
       "2  Factbox: Trump fills top jobs for his administ...  politicsNews   \n",
       "3  CNBC EDITOR: Media Must Remember Readers Are N...     left-news   \n",
       "4  NYC: Turkish Thugs Beat Up Protesters…Deny Fre...      politics   \n",
       "\n",
       "                 date label   fake  \n",
       "0      August 2, 2016  fake  False  \n",
       "1      April 1, 2016   real   True  \n",
       "2  November 29, 2016   real   True  \n",
       "3        Jun 29, 2017  fake  False  \n",
       "4        Sep 22, 2017  fake  False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[\"text\"] = news_df[\"title\"] + \": \" + news_df[\"text\"] \n",
    "news_df[\"fake\"] = news_df[\"label\"].apply(lambda x: True if x == 'real' else False)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSequenceModel():\n",
    "    \n",
    "    def __init__(self, pretrained_name = \"bert-base-uncased\"):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#berttokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "        \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        self.model = BertForSequenceClassification.from_pretrained(pretrained_name)\n",
    "        self.model.config.num_labels = 1\n",
    "        \n",
    "        # Freeze the pre trained parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        layers = [nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=1)]\n",
    "        \n",
    "        self.addLayers(layers)\n",
    "        \n",
    "    def addLayers(self, layers):\n",
    "        modules = []\n",
    "        \n",
    "        for layer in layers:\n",
    "            modules.append(layer)\n",
    "        \n",
    "        self.model.classifier = nn.Sequential(*modules)\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def preprocess_text_samples(self, samples, max_seq_length = 300):\n",
    "    \n",
    "        '''\n",
    "        Adapted from https://www.kaggle.com/clmentbisaillon/classifying-fake-news-with-bert/notebook\n",
    "        '''\n",
    "\n",
    "        encoded_samples = []\n",
    "        \n",
    "        for idx, sample in tqdm(samples.iterrows(), total = samples.shape[0]):\n",
    "            encoded_text = []\n",
    "            words = sample.text.strip().split(' ')\n",
    "            nb_seqs = int(len(words)/max_seq_length)\n",
    "\n",
    "            for i in range(nb_seqs+1):\n",
    "                words_part = ' '.join(words[i*max_seq_length : (i+1)*max_seq_length])\n",
    "\n",
    "                try:\n",
    "                    # https://huggingface.co/transformers/main_classes/tokenizer.html#pretrainedtokenizer\n",
    "                    # encoding using BERT pretrained tokeinizer and converts to pytorch tensors\n",
    "                    encoded_text.append(self.tokenizer.encode(words_part, return_tensors=\"pt\", \n",
    "                                                         max_length = 500, device = self.device))\n",
    "                except:\n",
    "                    print(\"Issue at: \" +str(idx))\n",
    "                    raise\n",
    "\n",
    "            encoded_samples.append(encoded_text)\n",
    "\n",
    "        return encoded_samples\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, nb_epochs = 10, log_freq = 500):\n",
    "        \n",
    "        train_loss_history = []\n",
    "        test_loss_history = []\n",
    "\n",
    "        criterion = nn.MSELoss().to(self.device)\n",
    "        optimizer = optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        print(\"------- Training started -------\\n\")\n",
    "        for epoch in range(nb_epochs):\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_accuracy = 0.0\n",
    "            test_accuracy = 0.0\n",
    "\n",
    "            '''\n",
    "            Iteration through training set\n",
    "            '''\n",
    "            self.model.train()\n",
    "\n",
    "            # iterate through the datapoints\n",
    "            for idx, text_tensor in enumerate(X_train):\n",
    "                # set gradients of all optimizers to zero -> avoid accumulation\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # define a tensor for the output \n",
    "                output = torch.zeros((1, 2)).float().to(self.device)\n",
    "\n",
    "                # iterate through each part of the text (each part is represented by a tensor)\n",
    "                # and obtain the average of the outputs\n",
    "                for i in range(len(text_tensor)):\n",
    "                    input = text_tensor[i]\n",
    "                    output += self.model(input, labels = y_train[idx])[1].float().to(self.device)\n",
    "\n",
    "                output = F.softmax(output[0], dim=-1)\n",
    "\n",
    "                # determine loss and accuracy\n",
    "                label = torch.tensor([1.0, 0.0]).float().to(self.device) if y_train[idx] == 0 else torch.tensor([0.0, 1.0]).float().to(self.device)\n",
    "                loss = criterion(output, label)\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                if label.max(0)[1] == output.max(0)[1]:\n",
    "                    train_accuracy += 1.0/len(X_train)\n",
    "\n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if log_freq and idx>0 and idx%log_freq == 0:\n",
    "                    print(\"Trained {}/{}: avg loss = {:.2f}\".format(idx, len(X_train), train_loss/log_freq))\n",
    "            \n",
    "            # iterate through test set\n",
    "            test_loss, test_accuracy = self.test_model(X_val, y_val, criterion)\n",
    "            \n",
    "            print(\">>>>>>> Epoch ({}/{}): train accuracy = {:.2f}%, test accuracy = {:.2f}%\\n\".format(epoch+1, nb_epochs, \n",
    "                                                                                                      train_accuracy*100, test_accuracy*100))\n",
    "            train_loss_history.append(train_loss)\n",
    "            test_loss_history.append(test_loss)\n",
    "\n",
    "        #save weights after training\n",
    "        print(\"------- End of training, saving weights -------\\n\")\n",
    "\n",
    "        if not os.path.exists(\"weights\"):\n",
    "            os.makedirs(\"weights/\")\n",
    "\n",
    "        torch.save(self.model.state_dict(), \"weights/model_\" + str(nb_epochs) + \"epochs_\" \n",
    "                   + str(np.round(train_accuracy*100, 2)) + \"train_\" + str(np.round(test_accuracy*100, 2)) + \"test\" + \".pt\")\n",
    "        \n",
    "        return train_loss_history, test_loss_history\n",
    "            \n",
    "    def test_model(self, X_test, y_test, criterion):\n",
    "        \n",
    "        '''\n",
    "        Iteration through test set\n",
    "        '''\n",
    "\n",
    "        # set layers to test/evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # set all grad flags to inactive\n",
    "        with torch.no_grad():\n",
    "            test_accuracy = 0.0\n",
    "            test_loss = 0.0\n",
    "\n",
    "            for idx, tensor in enumerate(X_test):\n",
    "\n",
    "                # define a tensor for the output \n",
    "                output = torch.zeros((1, 2)).float().to(self.device)\n",
    "\n",
    "                # iterate through each part of the text (each part is represented by a tensor)\n",
    "                # and obtain the average of the outputs\n",
    "                for text in tensor:\n",
    "                    output += self.model(text)[0].float().to(self.device)\n",
    "\n",
    "                output = F.softmax(output[0], dim=-1)\n",
    "\n",
    "                # determine loss and accuracy\n",
    "                label = torch.tensor([1.0, 0.0]).float().to(self.device) if y_test[idx] == 0 else torch.tensor([0.0, 1.0]).float().to(self.device)\n",
    "                loss = criterion(output, label)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                if label.max(0)[1] == output.max(0)[1]:\n",
    "                    test_accuracy += 1.0/len(X_test)\n",
    "                    \n",
    "        return test_loss, test_accuracy\n",
    "    \n",
    "    def predict():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BERTSequenceModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still in progress / testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 33.09it/s]\n"
     ]
    }
   ],
   "source": [
    "nb_samples = 25\n",
    "\n",
    "tensor_list = bert.preprocess_text_samples(news_df[:nb_samples])\n",
    "tensor_labels = news_df.fake[:nb_samples].apply(lambda x: torch.tensor([x]).long().to(bert.device)).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(tensor_list, tensor_labels, test_size=0.4, \n",
    "                                                    random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "torch.save(X_train, 'data/X_train.pt')\n",
    "torch.save(y_train, 'data/y_train.pt')\n",
    "torch.save(X_val, 'data/X_val.pt')\n",
    "torch.save(y_val, 'data/y_val.pt')\n",
    "torch.save(X_test, 'data/X_test.pt')\n",
    "torch.save(y_test, 'data/y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.load('data/X_train.pt')\n",
    "#y_train = torch.load('data/y_train.pt')\n",
    "#X_val = torch.load('data/X_val.pt')\n",
    "#y_val = torch.load('data/y_val.pt')\n",
    "#X_test = torch.load('data/X_test.pt')\n",
    "#y_test = torch.load('data/y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Training started -------\n",
      "\n",
      "Trained 5/15: avg loss = 0.29\n",
      "Trained 10/15: avg loss = 0.53\n",
      ">>>>>>> Epoch (1/2): train accuracy = 60.00%, test accuracy = 66.67%\n",
      "\n",
      "Trained 5/15: avg loss = 0.21\n",
      "Trained 10/15: avg loss = 0.44\n",
      ">>>>>>> Epoch (2/2): train accuracy = 60.00%, test accuracy = 66.67%\n",
      "\n",
      "------- End of training, saving weights -------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3.640628010034561, 3.3929563872516155],\n",
       " [1.5488757267594337, 1.5542257949709892])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_history, test_loss_history = bert.train_model(X_train, y_train, X_val, y_val, nb_epochs = 2, log_freq = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
